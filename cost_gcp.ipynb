{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"livescore-web\"\n",
    "BUCKET = \"cost_anomaly_detection\"\n",
    "REGION = \"europe-west1\"\n",
    "# ROOT_DIR = \"cost_anomaly_detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "# os.environ['ROOT_DIR'] = ROOT_DIR\n",
    "os.environ['TFVERSION'] = \"2.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil mb -l $REGION gs://$BUCKET # Create a bucket if there doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Blob {} downloaded to {}.\".format(\n",
    "            source_blob_name, destination_file_name\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ziye.zhou/Documents/Work/cost_anomaly_detection/tensorflow-venv/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/ziye.zhou/Documents/Work/cost_anomaly_detection/tensorflow-venv/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob preproc/cost_null_project.csv downloaded to ./cost.csv.\n"
     ]
    }
   ],
   "source": [
    "download_blob(\"cost_anomaly_detection\", \"preproc/cost_null_project.csv\", \"./cost.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cost': array([4.1900e+00, 1.0420e+01, 8.3900e+00, 1.1680e+01, 9.9400e+00,\n",
       "        9.9400e+00, 1.0040e+01, 9.3300e+00, 9.9400e+00, 9.5300e+00,\n",
       "        9.8400e+00, 9.8400e+00, 9.4300e+00, 1.0760e+01, 9.3300e+00,\n",
       "        1.1270e+01, 8.0800e+00, 1.4210e+01, 1.3750e+01, 9.0000e+00,\n",
       "        1.7150e+01, 1.6530e+01, 1.9330e+01, 1.0820e+01, 1.1880e+01,\n",
       "        1.1550e+01, 3.0440e+01, 3.9590e+01, 2.3500e+01, 3.7640e+01,\n",
       "        4.5870e+01, 4.2930e+01, 3.8640e+01, 1.4066e+02, 1.6766e+02,\n",
       "        6.7270e+01, 3.1520e+01, 3.1820e+01, 3.0470e+01, 4.0460e+01,\n",
       "        6.7840e+01, 7.1500e+01, 3.9850e+01, 3.8600e+01, 2.5290e+01,\n",
       "        4.5340e+01, 2.1200e+01, 8.8790e+01, 1.2041e+02, 7.7400e+01,\n",
       "        3.3150e+01, 6.0130e+01, 6.9050e+01, 4.8160e+01, 1.1256e+02,\n",
       "        1.5001e+02, 6.1530e+01, 3.0570e+01, 4.9230e+01, 8.3990e+01,\n",
       "        3.8290e+01, 1.1744e+02, 1.0641e+02, 6.9170e+01, 5.7880e+01,\n",
       "        9.9730e+01, 9.5250e+01, 5.9990e+01, 6.0670e+01, 1.8634e+02,\n",
       "        1.4864e+02, 7.1200e+01, 4.7870e+01, 2.7480e+01, 2.9260e+01,\n",
       "        7.8560e+01, 5.4120e+01, 4.3030e+01, 3.3770e+01, 2.1860e+01,\n",
       "        4.5950e+01, 2.4560e+01, 1.3148e+02, 1.0891e+02, 4.9110e+01,\n",
       "        4.6860e+01, 4.8240e+01, 7.4860e+01, 4.7040e+01, 9.8270e+01,\n",
       "        1.4362e+02, 5.4580e+01, 4.2650e+01, 3.8320e+01, 7.3230e+01,\n",
       "        7.2790e+01, 1.6830e+02, 2.3108e+02, 8.1540e+01, 6.2760e+01,\n",
       "        8.4760e+01, 8.5240e+01, 5.2180e+01, 9.5270e+01, 1.4110e+02,\n",
       "        8.1610e+01, 2.6680e+01, 4.0820e+01, 3.1390e+01, 4.4440e+01,\n",
       "        6.3190e+01, 5.4960e+01, 4.7050e+01, 3.0400e+01, 5.1160e+01,\n",
       "        3.2070e+01, 3.0940e+01, 9.8540e+01, 1.1631e+02, 5.9510e+01,\n",
       "        4.4770e+01, 6.0050e+01, 6.9120e+01, 5.1780e+01, 1.4034e+02,\n",
       "        2.0507e+02, 7.0130e+01, 6.7330e+01, 6.2380e+01, 1.0406e+02,\n",
       "        4.4190e+01, 1.3579e+02, 1.2594e+02, 5.2630e+01, 4.7780e+01,\n",
       "        7.4260e+01, 6.3770e+01, 5.3230e+01, 9.4230e+01, 1.3253e+02,\n",
       "        5.3920e+01, 2.8150e+01, 5.4050e+01, 4.7920e+01, 3.1870e+01,\n",
       "        1.1782e+02, 8.8600e+01, 3.6460e+01, 2.6620e+01, 8.1590e+01,\n",
       "        5.0690e+01, 3.1980e+01, 8.4400e+01, 4.2960e+01, 5.9220e+01,\n",
       "        6.9360e+01, 6.4520e+01, 7.2860e+01, 6.9410e+01, 1.0820e+02,\n",
       "        9.0720e+01, 6.2220e+01, 6.3060e+01, 6.2890e+01, 6.1500e+01,\n",
       "        3.3970e+01, 1.1859e+02, 1.1078e+02, 5.9690e+01, 3.5250e+01,\n",
       "        5.6400e+01, 5.6230e+01, 3.8120e+01, 1.0752e+02, 1.7443e+02,\n",
       "        7.4190e+01, 5.3120e+01, 5.3970e+01, 4.6070e+01, 6.2230e+01,\n",
       "        1.0558e+02, 1.2924e+02, 5.7310e+01, 6.3550e+01, 8.8260e+01,\n",
       "        9.5630e+01, 6.2780e+01, 8.6040e+01, 3.6513e+02, 1.1265e+02,\n",
       "        8.6050e+01, 5.7370e+01, 8.0750e+01, 5.8210e+01, 1.6298e+02,\n",
       "        1.9080e+02, 6.9920e+01, 5.7430e+01, 4.5110e+01, 1.0442e+02,\n",
       "        8.2480e+01, 1.0550e+02, 1.6744e+02, 3.5860e+01, 7.4980e+01,\n",
       "        8.0700e+01, 8.1330e+01, 7.0870e+01, 1.1170e+02, 1.4402e+02,\n",
       "        5.5910e+01, 4.7290e+01, 5.4450e+01, 8.9150e+01, 7.7230e+01,\n",
       "        5.6830e+01, 2.9073e+02, 2.2322e+02, 7.3850e+01, 8.4930e+01,\n",
       "        8.3530e+01, 8.1670e+01, 1.5326e+02, 1.5630e+02, 7.2110e+01,\n",
       "        5.5610e+01, 9.2510e+01, 7.7610e+01, 6.4040e+01, 1.4816e+02,\n",
       "        8.4030e+01, 1.4777e+02, 4.1140e+01, 3.9680e+01, 1.8320e+01,\n",
       "        6.8060e+01, 7.8200e+01, 8.1730e+01, 4.7050e+01, 3.1050e+01,\n",
       "        8.3600e+01, 4.3100e+01, 7.5080e+02, 1.2976e+02, 1.5100e+01,\n",
       "        1.7211e+02, 1.1025e+02, 1.3401e+02, 7.6370e+01, 2.1845e+02,\n",
       "        1.4591e+02, 6.6360e+01, 5.7720e+01, 4.4179e+02, 6.4590e+01,\n",
       "        1.6491e+02, 4.2522e+02, 5.7590e+01, 7.6690e+01, 7.1880e+01,\n",
       "        7.6750e+01, 1.5122e+02, 1.4568e+02, 4.7100e+01, 8.8180e+01,\n",
       "        7.5080e+01, 8.8080e+01, 6.1980e+01, 1.5385e+02, 1.4579e+02,\n",
       "        4.5370e+01, 3.0250e+01, 6.4360e+01, 8.9680e+01, 1.0000e-02,\n",
       "        9.5260e+01, 3.1876e+02, 2.3033e+02, 7.4360e+01, 7.3220e+01,\n",
       "        7.6120e+01, 4.0560e+01, 6.0220e+01, 1.4503e+02, 1.8690e+02,\n",
       "        6.1080e+01, 5.5030e+01, 6.9990e+01, 4.7070e+01, 1.0958e+02,\n",
       "        8.6690e+01, 7.9270e+01, 5.1610e+01, 4.3120e+01, 4.9530e+01,\n",
       "        5.6440e+01, 7.4770e+01, 8.2080e+01, 5.6770e+01, 4.2200e+01,\n",
       "        1.0630e+01, 5.3000e+01, 8.6400e+01, 5.8410e+01, 1.1242e+02,\n",
       "        8.7530e+01, 7.1610e+01, 6.7730e+01, 5.7430e+01, 9.3090e+01,\n",
       "        8.3980e+01, 9.9960e+01, 4.8070e+01, 4.9630e+01, 6.3120e+01,\n",
       "        2.1490e+01, 1.0147e+02, 5.6320e+01, 5.6040e+01, 5.8150e+01,\n",
       "        4.8830e+01, 5.0130e+01, 4.8320e+01, 4.5980e+01, 5.5920e+01,\n",
       "        5.9260e+01, 5.0900e+01, 4.5350e+01, 4.9550e+01, 3.7790e+01,\n",
       "        4.8980e+01, 5.3600e+01, 5.0560e+01, 5.8980e+01, 6.6190e+01,\n",
       "        9.1970e+01, 6.7380e+01, 7.6380e+01, 8.9480e+01, 8.5350e+01,\n",
       "        7.3800e+01, 5.3730e+01, 7.3440e+01, 5.9470e+01, 5.7400e+01,\n",
       "        4.6210e+01, 7.5520e+01, 6.4760e+01, 4.6560e+01, 4.3850e+01,\n",
       "        5.0480e+01, 6.6530e+01, 6.0780e+01, 4.0480e+01, 4.1140e+01,\n",
       "        4.5180e+01, 5.0610e+01, 3.7980e+01, 6.3090e+01, 5.9810e+01,\n",
       "        3.9380e+01, 3.9950e+01, 4.1790e+01, 6.1340e+01, 9.4710e+01,\n",
       "        1.4871e+02, 1.2201e+02, 8.2810e+01, 7.0480e+01, 8.3010e+01,\n",
       "        8.4090e+01, 5.8250e+01, 1.6506e+02, 1.2739e+02, 6.7980e+01,\n",
       "        5.9570e+01, 6.8600e+01, 5.5770e+01, 8.7730e+01, 6.2830e+01,\n",
       "        2.5686e+02, 6.6250e+01, 6.1770e+01, 5.2530e+01, 4.7010e+01,\n",
       "        7.6640e+01, 1.2406e+02, 2.3635e+02, 9.7010e+01, 4.8730e+01,\n",
       "        7.0630e+01, 6.2400e+01, 6.9760e+01, 2.1396e+02, 2.3939e+02,\n",
       "        1.4248e+02, 8.3450e+01, 9.9300e+01, 9.5000e+01, 1.0659e+02,\n",
       "        1.6052e+02, 1.2781e+02, 8.0660e+01, 8.3700e+01, 9.3110e+01,\n",
       "        6.1480e+01, 7.7950e+01, 2.2623e+02, 2.2725e+02, 9.2530e+01,\n",
       "        9.5660e+01, 1.1333e+02, 1.1226e+02, 9.5930e+01, 2.2207e+02,\n",
       "        2.8935e+02, 8.8290e+01, 7.9680e+01, 8.2400e+01, 1.0337e+02,\n",
       "        5.8960e+01, 1.9129e+02, 1.8752e+02, 6.4070e+01, 1.0937e+02,\n",
       "        2.2036e+02, 2.0350e+02, 1.4814e+02, 3.2571e+02, 2.8208e+02,\n",
       "        1.3706e+02, 8.4500e+01, 8.3830e+01, 9.5370e+01, 9.7280e+01,\n",
       "        1.3255e+02, 1.2885e+02, 1.4898e+02, 9.9400e+01, 9.6670e+01,\n",
       "        8.7830e+01, 9.1120e+01, 2.2938e+02, 2.0791e+02, 7.2090e+01,\n",
       "        7.4620e+01, 9.2930e+01, 9.3570e+01, 7.4800e+01, 1.5173e+02,\n",
       "        1.6531e+02, 5.9140e+01, 3.0130e+01, 9.2040e+01, 1.4051e+02,\n",
       "        9.6900e+01, 3.4628e+02, 3.0240e+02, 1.2187e+02, 9.7630e+01,\n",
       "        1.5162e+02, 1.3805e+02, 1.2080e+02, 2.0439e+02, 2.1520e+02,\n",
       "        1.6077e+02, 8.3150e+01, 7.9710e+01, 8.9170e+01, 1.0187e+02,\n",
       "        1.5840e+01, 1.6806e+02, 1.2853e+02, 8.5920e+01, 7.7200e+01,\n",
       "        8.3880e+01, 8.4740e+01, 1.5191e+02, 1.5658e+02, 5.9090e+01,\n",
       "        6.0580e+01, 9.5120e+01, 1.0512e+02, 7.8260e+01, 1.4777e+02,\n",
       "        2.7756e+02, 1.5726e+02, 1.4481e+02, 1.7199e+02, 1.4530e+02,\n",
       "        1.3875e+02, 2.6000e+02, 1.9639e+02, 1.0520e+02, 1.0276e+02,\n",
       "        1.4502e+02, 8.5830e+01, 1.6575e+02, 2.1027e+02, 1.8834e+02,\n",
       "        9.8600e+01, 5.0510e+01, 1.1619e+02, 1.2600e+02, 6.5710e+01,\n",
       "        1.4453e+02, 1.1410e+02, 7.0980e+01, 6.2040e+01, 5.5900e+01,\n",
       "        8.3700e+01, 7.5630e+01, 9.3080e+01, 9.0590e+01, 6.0930e+01,\n",
       "        5.5040e+01, 1.5218e+02, 1.4076e+02, 1.4836e+02, 1.9914e+02,\n",
       "        1.6039e+02, 1.0852e+02, 1.1148e+02, 1.1700e+02, 9.2720e+01,\n",
       "        9.1490e+01, 9.8200e+01, 9.0050e+01, 8.8010e+01, 1.7534e+02,\n",
       "        1.7378e+02, 1.1539e+02, 8.5730e+01, 1.9019e+02, 1.6718e+02,\n",
       "        1.0237e+02, 1.0598e+02, 6.9020e+01, 1.0010e+02, 6.2590e+01,\n",
       "        1.1773e+02, 1.0474e+02, 6.5480e+01, 6.0970e+01, 5.8380e+01,\n",
       "        8.9950e+01, 7.5030e+01, 2.6195e+02, 2.4740e+02, 1.5586e+02,\n",
       "        1.4148e+02, 1.3183e+02, 1.0695e+02, 1.4916e+02, 1.8660e+02,\n",
       "        1.6724e+02, 1.1288e+02, 1.1650e+02, 1.3474e+02, 1.2684e+02,\n",
       "        1.1682e+02, 2.0527e+02, 2.1148e+02, 1.3952e+02, 9.4940e+01,\n",
       "        9.6520e+01, 8.4580e+01, 1.1749e+02, 1.5338e+02, 1.8508e+02,\n",
       "        9.1810e+01, 7.0240e+01, 9.9690e+01, 1.0121e+02, 1.2640e+02,\n",
       "        1.7216e+02, 3.0914e+02, 2.2703e+02, 2.2299e+02, 2.0181e+02,\n",
       "        1.7319e+02, 2.4464e+02, 7.0360e+01, 5.1738e+02, 1.0896e+02,\n",
       "        1.7864e+02, 1.2104e+02, 1.8923e+02, 1.6169e+02, 1.6067e+02,\n",
       "        1.0194e+02, 8.3190e+01, 6.2500e+01, 7.5460e+01, 9.5240e+01,\n",
       "        9.2350e+01, 8.6250e+01, 8.0650e+01, 8.0350e+01, 7.8380e+01,\n",
       "        6.9470e+01, 9.6620e+01, 7.9850e+01, 7.2260e+01, 6.9340e+01,\n",
       "        6.1190e+01, 7.6600e+01, 1.2323e+02, 1.8503e+02, 1.8707e+02,\n",
       "        1.9554e+02, 1.4663e+02, 1.2767e+02, 1.5578e+02, 1.9277e+02,\n",
       "        1.0909e+02, 9.4900e+00, 9.5900e+00, 9.4900e+00, 9.3000e+00,\n",
       "        9.6900e+00, 9.4000e+00, 9.4900e+00, 9.2000e+00, 9.7900e+00,\n",
       "        9.4000e+00, 9.4900e+00, 9.4900e+00, 9.6900e+00, 8.9000e+00,\n",
       "        1.0090e+01, 9.3000e+00, 9.5900e+00, 9.3000e+00, 9.4900e+00,\n",
       "        9.4000e+00, 9.8800e+00, 8.9700e+00, 9.5400e+00, 8.3100e+00,\n",
       "        8.9700e+00, 5.2900e+00], dtype=float32),\n",
       " 'noCost': array([4.100e+01, 1.020e+02, 8.200e+01, 1.140e+02, 9.700e+01, 9.700e+01,\n",
       "        9.800e+01, 9.100e+01, 9.700e+01, 9.300e+01, 9.600e+01, 9.600e+01,\n",
       "        9.200e+01, 1.050e+02, 9.000e+01, 8.900e+01, 4.400e+01, 6.500e+01,\n",
       "        8.000e+01, 6.000e+01, 6.000e+01, 6.600e+01, 6.200e+01, 6.200e+01,\n",
       "        6.000e+01, 5.400e+01, 6.100e+01, 6.300e+01, 6.200e+01, 7.300e+01,\n",
       "        5.700e+01, 6.500e+01, 5.800e+01, 7.200e+01, 6.600e+01, 5.400e+01,\n",
       "        5.700e+01, 4.300e+01, 5.600e+01, 6.200e+01, 5.600e+01, 6.300e+01,\n",
       "        6.300e+01, 6.000e+01, 5.300e+01, 6.300e+01, 5.900e+01, 8.200e+01,\n",
       "        7.100e+01, 7.800e+01, 5.900e+01, 7.100e+01, 7.200e+01, 7.100e+01,\n",
       "        7.900e+01, 7.500e+01, 6.300e+01, 6.900e+01, 5.400e+01, 6.600e+01,\n",
       "        6.100e+01, 6.900e+01, 7.000e+01, 7.200e+01, 5.600e+01, 5.600e+01,\n",
       "        5.100e+01, 5.100e+01, 4.500e+01, 7.800e+01, 6.000e+01, 9.200e+01,\n",
       "        8.800e+01, 6.800e+01, 5.500e+01, 9.300e+01, 7.900e+01, 8.000e+01,\n",
       "        5.600e+01, 6.300e+01, 6.500e+01, 6.600e+01, 7.000e+01, 8.500e+01,\n",
       "        7.300e+01, 7.300e+01, 6.000e+01, 6.000e+01, 6.700e+01, 6.600e+01,\n",
       "        6.500e+01, 6.200e+01, 6.700e+01, 4.900e+01, 6.800e+01, 5.200e+01,\n",
       "        5.100e+01, 7.500e+01, 5.500e+01, 5.600e+01, 3.300e+01, 5.800e+01,\n",
       "        3.200e+01, 5.000e+01, 5.700e+01, 6.000e+01, 5.200e+01, 5.300e+01,\n",
       "        5.800e+01, 5.500e+01, 5.900e+01, 5.400e+01, 6.600e+01, 5.500e+01,\n",
       "        6.600e+01, 6.600e+01, 5.700e+01, 6.500e+01, 5.900e+01, 5.900e+01,\n",
       "        5.800e+01, 5.200e+01, 5.000e+01, 4.600e+01, 9.100e+01, 5.600e+01,\n",
       "        5.200e+01, 5.200e+01, 4.300e+01, 4.300e+01, 4.500e+01, 7.100e+01,\n",
       "        6.600e+01, 7.700e+01, 8.300e+01, 8.100e+01, 7.700e+01, 8.500e+01,\n",
       "        7.900e+01, 8.700e+01, 8.600e+01, 7.700e+01, 8.600e+01, 8.600e+01,\n",
       "        7.900e+01, 8.900e+01, 8.600e+01, 8.300e+01, 8.800e+01, 8.400e+01,\n",
       "        8.700e+01, 8.500e+01, 8.200e+01, 4.400e+01, 8.900e+01, 9.200e+01,\n",
       "        9.500e+01, 8.800e+01, 8.400e+01, 7.900e+01, 8.000e+01, 8.200e+01,\n",
       "        8.400e+01, 8.400e+01, 7.800e+01, 6.300e+01, 9.400e+01, 7.900e+01,\n",
       "        7.400e+01, 6.500e+01, 9.600e+01, 8.600e+01, 8.200e+01, 8.700e+01,\n",
       "        8.100e+01, 7.900e+01, 8.100e+01, 8.900e+01, 7.400e+01, 1.000e+02,\n",
       "        7.800e+01, 8.400e+01, 6.700e+01, 9.800e+01, 7.400e+01, 7.200e+01,\n",
       "        9.100e+01, 6.300e+01, 1.140e+02, 7.400e+01, 7.800e+01, 7.100e+01,\n",
       "        7.400e+01, 7.100e+01, 6.500e+01, 7.200e+01, 7.200e+01, 8.600e+01,\n",
       "        2.000e+01, 1.270e+02, 1.110e+02, 9.100e+01, 8.400e+01, 4.100e+01,\n",
       "        8.100e+01, 9.800e+01, 9.200e+01, 9.400e+01, 9.700e+01, 8.400e+01,\n",
       "        8.100e+01, 7.200e+01, 9.200e+01, 8.500e+01, 7.800e+01, 5.000e+01,\n",
       "        7.800e+01, 1.070e+02, 9.000e+01, 7.400e+01, 9.100e+01, 8.100e+01,\n",
       "        9.500e+01, 8.600e+01, 7.700e+01, 8.600e+01, 8.300e+01, 8.300e+01,\n",
       "        6.300e+01, 9.600e+01, 6.700e+01, 9.300e+01, 7.400e+01, 7.100e+01,\n",
       "        3.400e+01, 1.210e+02, 9.400e+01, 9.200e+01, 6.800e+01, 4.300e+01,\n",
       "        1.390e+02, 9.000e+01, 3.650e+02, 6.900e+01, 1.900e+01, 1.560e+02,\n",
       "        8.800e+01, 8.600e+01, 8.700e+01, 9.100e+01, 8.100e+01, 7.300e+01,\n",
       "        8.100e+01, 4.870e+02, 1.611e+03, 9.000e+01, 4.970e+02, 1.403e+03,\n",
       "        8.900e+01, 8.000e+01, 8.200e+01, 7.900e+01, 6.400e+01, 4.700e+01,\n",
       "        6.200e+01, 1.010e+02, 7.200e+01, 6.400e+01, 7.400e+01, 7.200e+01,\n",
       "        6.500e+01, 8.900e+01, 9.300e+01, 8.400e+01, 1.000e+00, 7.600e+01,\n",
       "        1.420e+02, 9.600e+01, 9.300e+01, 9.000e+01, 9.500e+01, 3.600e+01,\n",
       "        6.700e+01, 1.070e+02, 9.200e+01, 9.800e+01, 8.300e+01, 9.400e+01,\n",
       "        8.400e+01, 8.800e+01, 6.900e+01, 6.800e+01, 7.700e+01, 8.500e+01,\n",
       "        7.600e+01, 8.200e+01, 8.200e+01, 9.100e+01, 6.700e+01, 8.200e+01,\n",
       "        5.000e+00, 2.900e+01, 9.500e+01, 9.100e+01, 8.700e+01, 9.400e+01,\n",
       "        9.700e+01, 9.300e+01, 8.000e+01, 1.090e+02, 9.200e+01, 9.600e+01,\n",
       "        7.400e+01, 9.600e+01, 8.000e+01, 4.400e+01, 1.250e+02, 9.700e+01,\n",
       "        9.600e+01, 8.700e+01, 8.800e+01, 9.200e+01, 9.600e+01, 9.100e+01,\n",
       "        9.100e+01, 9.000e+01, 8.500e+01, 7.600e+01, 9.300e+01, 9.400e+01,\n",
       "        8.700e+01, 9.300e+01, 8.900e+01, 9.300e+01, 7.500e+01, 8.800e+01,\n",
       "        9.400e+01, 7.300e+01, 9.800e+01, 7.100e+01, 8.100e+01, 6.000e+01,\n",
       "        9.600e+01, 6.800e+01, 1.000e+02, 9.600e+01, 9.600e+01, 8.600e+01,\n",
       "        8.800e+01, 8.800e+01, 1.040e+02, 9.300e+01, 9.700e+01, 8.800e+01,\n",
       "        8.500e+01, 8.600e+01, 1.020e+02, 9.200e+01, 9.600e+01, 9.700e+01,\n",
       "        8.300e+01, 8.900e+01, 9.600e+01, 1.050e+02, 1.030e+02, 9.600e+01,\n",
       "        9.200e+01, 9.100e+01, 9.100e+01, 9.600e+01, 9.500e+01, 9.600e+01,\n",
       "        9.700e+01, 9.100e+01, 9.600e+01, 9.300e+01, 9.600e+01, 7.700e+01,\n",
       "        1.170e+02, 9.500e+01, 8.700e+01, 9.300e+01, 9.400e+01, 9.400e+01,\n",
       "        8.700e+01, 9.600e+01, 7.600e+01, 1.050e+02, 8.800e+01, 9.800e+01,\n",
       "        9.400e+01, 9.500e+01, 9.800e+01, 9.200e+01, 1.110e+02, 1.050e+02,\n",
       "        8.800e+01, 9.200e+01, 9.600e+01, 9.300e+01, 1.000e+02, 9.400e+01,\n",
       "        9.700e+01, 9.200e+01, 9.800e+01, 8.900e+01, 1.020e+02, 9.500e+01,\n",
       "        9.100e+01, 9.100e+01, 1.020e+02, 9.500e+01, 9.600e+01, 9.500e+01,\n",
       "        9.700e+01, 9.700e+01, 9.100e+01, 9.800e+01, 9.600e+01, 9.400e+01,\n",
       "        9.700e+01, 9.500e+01, 9.800e+01, 8.800e+01, 1.070e+02, 1.040e+02,\n",
       "        9.800e+01, 8.600e+01, 1.060e+02, 9.700e+01, 9.300e+01, 9.200e+01,\n",
       "        9.000e+01, 9.400e+01, 9.700e+01, 1.000e+02, 9.300e+01, 9.300e+01,\n",
       "        9.300e+01, 9.600e+01, 1.020e+02, 9.300e+01, 9.400e+01, 9.500e+01,\n",
       "        9.600e+01, 9.900e+01, 9.500e+01, 9.500e+01, 9.600e+01, 9.600e+01,\n",
       "        9.500e+01, 6.300e+01, 4.700e+01, 1.200e+02, 1.520e+02, 1.060e+02,\n",
       "        1.060e+02, 9.200e+01, 8.800e+01, 9.300e+01, 1.030e+02, 9.300e+01,\n",
       "        9.700e+01, 9.700e+01, 8.900e+01, 9.600e+01, 9.800e+01, 6.200e+01,\n",
       "        7.000e+01, 6.300e+01, 1.100e+01, 1.010e+02, 7.300e+01, 6.700e+01,\n",
       "        5.500e+01, 3.400e+01, 7.600e+01, 5.500e+01, 6.400e+01, 5.700e+01,\n",
       "        5.100e+01, 5.300e+01, 6.200e+01, 5.900e+01, 6.100e+01, 1.060e+02,\n",
       "        7.400e+01, 6.000e+01, 6.200e+01, 5.400e+01, 7.300e+01, 7.700e+01,\n",
       "        7.200e+01, 6.300e+01, 7.400e+01, 6.400e+01, 1.900e+01, 6.700e+01,\n",
       "        7.400e+01, 7.500e+01, 6.800e+01, 3.900e+01, 3.200e+01, 5.800e+01,\n",
       "        5.500e+01, 7.400e+01, 7.300e+01, 6.600e+01, 7.000e+01, 7.600e+01,\n",
       "        7.000e+01, 7.100e+01, 7.400e+01, 7.900e+01, 6.900e+01, 7.600e+01,\n",
       "        1.000e+02, 8.000e+01, 5.900e+01, 7.200e+01, 6.900e+01, 5.700e+01,\n",
       "        6.500e+01, 6.200e+01, 5.400e+01, 6.000e+01, 6.800e+01, 6.600e+01,\n",
       "        6.500e+01, 6.200e+01, 5.800e+01, 6.400e+01, 6.900e+01, 6.900e+01,\n",
       "        7.000e+01, 6.900e+01, 6.800e+01, 6.100e+01, 6.900e+01, 5.000e+01,\n",
       "        7.300e+01, 7.200e+01, 6.600e+01, 6.400e+01, 6.200e+01, 5.600e+01,\n",
       "        6.200e+01, 6.700e+01, 8.500e+01, 6.100e+01, 6.300e+01, 5.600e+01,\n",
       "        5.900e+01, 6.300e+01, 7.100e+01, 7.700e+01, 7.400e+01, 6.700e+01,\n",
       "        7.100e+01, 7.000e+01, 7.000e+01, 6.800e+01, 7.600e+01, 7.400e+01,\n",
       "        5.000e+01, 5.900e+01, 5.300e+01, 4.600e+01, 6.500e+01, 6.700e+01,\n",
       "        6.000e+01, 4.600e+01, 5.300e+01, 5.000e+01, 6.100e+01, 6.400e+01,\n",
       "        3.680e+02, 8.000e+01, 5.600e+01, 4.400e+01, 4.000e+01, 5.600e+01,\n",
       "        2.400e+01, 1.090e+02, 4.300e+01, 7.100e+01, 6.800e+01, 7.500e+01,\n",
       "        8.600e+01, 7.300e+01, 6.100e+01, 6.400e+01, 3.800e+01, 3.600e+01,\n",
       "        6.000e+01, 5.500e+01, 6.300e+01, 6.200e+01, 5.900e+01, 5.000e+01,\n",
       "        5.100e+01, 6.400e+01, 5.400e+01, 6.200e+01, 7.100e+01, 5.200e+01,\n",
       "        5.800e+01, 3.560e+02, 5.500e+01, 4.000e+01, 3.500e+01, 3.300e+01,\n",
       "        4.000e+01, 3.400e+01, 5.400e+01, 6.100e+01, 1.080e+02, 1.100e+02,\n",
       "        1.200e+02, 1.140e+02, 1.120e+02, 1.020e+02, 1.100e+02, 1.060e+02,\n",
       "        1.180e+02, 1.140e+02, 1.260e+02, 1.000e+02, 1.160e+02, 9.000e+01,\n",
       "        1.040e+02, 1.080e+02, 1.200e+02, 1.200e+02, 1.080e+02, 8.800e+01,\n",
       "        4.170e+02, 1.160e+02, 1.180e+02, 9.600e+01, 1.020e+02, 5.200e+01],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data_csv(filepath, header_included = True):\n",
    "    \n",
    "    import csv\n",
    "    import numpy as np\n",
    "    \n",
    "    data_dict = {}\n",
    "    with open(filepath, newline = \"\") as csvfile:\n",
    "        data = list(csv.reader(csvfile))\n",
    "    header = data[0]\n",
    "    data = np.array(data[1:], dtype = np.float32) if header_included else np.array(data, dtype = np.float32)\n",
    "    for index, name in enumerate(header):\n",
    "        data_dict[name] = data[:, index]\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "read_data_csv(\"cost.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from . import model\n",
    "from . import preprocess\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--bucket\",\n",
    "        help = \"GCS path to data. We assume that data is in gs://BUCKET/cost_anomaly_detection/preprocess/\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help = \"GCS location to write checkpoints and export models\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_split\",\n",
    "        help = \"Number of examples to train the model.\",\n",
    "        type = int,\n",
    "        default = 595\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help = \"Number of examples to compute gradient over.\",\n",
    "        type = int,\n",
    "        default = 5\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sequence_length\",\n",
    "        help = \"Number of historical examples to predict.\",\n",
    "        type = int,\n",
    "        default = 7\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--extract_mode\",\n",
    "        help = \"Method to feature extraction from raw features -- provide raw or ensemble.\",\n",
    "        type = str,\n",
    "        default = \"raw\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs_encoder\",\n",
    "        help = \"Number of epochs for training LSTM encoder.\",\n",
    "        type = int,\n",
    "        default = 100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs_forecaster\",\n",
    "        help = \"Number of epochs for training LSTM forecaster.\",\n",
    "        type = int,\n",
    "        default = 50\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help = 'this model ignores this field, but it is required by gcloud',\n",
    "        default = \"junk\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--encode_wide\",\n",
    "        help = \"Hidden layer sizes to use for LSTM encoder feature columns -- provide space-separated layers\",\n",
    "        nargs = '+',\n",
    "        type = int,\n",
    "        default = [128]\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--decode_wide\",\n",
    "        help = \"Hidden layer sizes to use for LSTM decoder feature columns -- provide space-separated layers\",\n",
    "        nargs = '+',\n",
    "        type = int,\n",
    "        default = [32]\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--forecaster_wide\",\n",
    "        help = \"Hidden layer sizes in LSTM cell to use for LSTM forecaster feature columns -- \\\n",
    "        provide space-separated layers\",\n",
    "        nargs = '+',\n",
    "        type = int,\n",
    "        default = [400, 200]\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dense_size\",\n",
    "        help = \"Hidden layer sizes to use for LSTM forecaster feature columns\",\n",
    "        type = int,\n",
    "        default = 200\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pattern\",\n",
    "        help = \"Specify a pattern that has to be in input files. For example 00001-of will process only one shard\",\n",
    "        default = \"of\"\n",
    "    )\n",
    "    \n",
    "    ## parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # unused args provided by service\n",
    "    arguments.pop(\"job_dir\", None)\n",
    "\n",
    "    ## assign the arguments to the model and preprocess variables\n",
    "    output_dir = arguments.pop(\"output_dir\")\n",
    "    preprocess.bucket = arguments.pop(\"bucket\")\n",
    "    preprocess.train_split = arguments.pop(\"train_split\")\n",
    "    preprocess.sequence_length = arguments.pop(\"sequence_length\")\n",
    "    preprocess.batch_size = arguments.pop(\"batch_size\")\n",
    "    preprocess.pattern = arguments.pop(\"pattern\")\n",
    "    model.bucket = arguments.pop(\"bucket')\n",
    "    model.batch_size = arguments.pop(\"batch_size\")\n",
    "    model.sequence_length = arguments.pop(\"sequence_length\")\n",
    "    model.extract_mode = arguments.pop(\"extract_mode\")\n",
    "    model.epochs_encoder = arguments.pop(\"epochs_encoder\")\n",
    "    model.epochs_forecaster = arguments.pop(\"epochs_forecaster\")\n",
    "    model.encode_wide = arguments.pop(\"encode_wide\")\n",
    "    model.decode_wide = arguments.pop(\"decode_wide\")\n",
    "    model.forecaster_wide = arguments.pop(\"forecaster_wide\")\n",
    "    model.dense_size = arguments.pop(\"dense_size\")  \n",
    "    model.pattern = arguments.pop(\"pattern\")\n",
    "\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    output_dir = os.path.join(\n",
    "        output_dir,\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    dataset_train, dataset_test, tensor_to_extract, tensor_to_eval, label_train, label_test = preprocess.load_data()\n",
    "    lstm_encoder = model.deploy_lstm_autoencoder(dataset_train, dataset_test, output_dir)\n",
    "    model.deploy_lstm_forecaster(\n",
    "        tensor_to_extract, tensor_to_eval, lstm_encoder, label_train, label_test, output_dir \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/preprocess.py\n",
    "\n",
    "import shutil, os, datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "bucket = None  # set from task.py\n",
    "pattern = \"of\" # gets all files\n",
    "\n",
    "feature_columns = [\"cost\", \"noCost\"]\n",
    "label_column = \"cost\"\n",
    "defaults = [[0.0], [0.0]]\n",
    "\n",
    "# Define some hyperparamters\n",
    "sequence_length = 7\n",
    "train_split = 595\n",
    "num_features = len(feature_columns)\n",
    "batch_size = 5\n",
    "buffer_size = 1000        \n",
    "\n",
    "\n",
    "# Create a read-csv-data function to read data from GCP Platform storage\n",
    "def read_data_csv(filepath, header_included = True):\n",
    "    \n",
    "    import csv\n",
    "    import numpy as np\n",
    "    \n",
    "    data_dict = {}\n",
    "    with open(filepath, newline = \"\") as csvfile:\n",
    "        data = list(csv.reader(csvfile))\n",
    "    header = data[0]\n",
    "    data = np.array(data[1:], dtype = np.float32) if header_included else np.array(data, dtype = np.float32)\n",
    "    for index, name in enumerate(header):\n",
    "        data_dict[name] = data[:, index]\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "# Create a function to generate sequence and label prepared for LSTM model\n",
    "def get_sequence(data, sequence_length):\n",
    "    \n",
    "    sequence = []\n",
    "    num_elements = data.shape[0]\n",
    "\n",
    "    for start, end in zip(range(0, num_elements-sequence_length), range(sequence_length, num_elements)):\n",
    "        sequence.append(data[start:end])\n",
    "    sequence = np.asarray(sequence).reshape((num_elements-sequence_length, sequence_length, 1))\n",
    "    return sequence\n",
    "\n",
    "def get_label(data, sequence_length = 7):\n",
    "\n",
    "    num_elements = data.shape[0]\n",
    "    label = np.asarray(data[sequence_length:num_elements])\n",
    "    label = label.reshape((data[sequence_length:num_elements].shape[0], 1))\n",
    "\n",
    "    return label\n",
    "\n",
    "# Create a function to preprocess our data by using function above\n",
    "def min_max_scaler(tensor):\n",
    "    \n",
    "    tensor_train_min = tf.math.reduce_min(tensor)\n",
    "    tensor_train_max = tf.math.reduce_max(tensor)\n",
    "    tensor = tf.math.divide(\n",
    "        tf.math.subtract(tensor, tensor_train_min), \n",
    "        tf.math.subtract(tensor_train_max, tensor_train_min)\n",
    "    )\n",
    "    \n",
    "    return tensor, tensor_train_min, tensor_train_max\n",
    "\n",
    "def preprocess_data(data, feature_columns, label_column, train_split, sequence_length):\n",
    "    \n",
    "    num_elements = len(data[label_column])\n",
    "    feature_train = []\n",
    "    feature_test = []\n",
    "    scaler_parameter = {}\n",
    "    \n",
    "    for key in feature_columns:\n",
    "        tensor_train = tf.convert_to_tensor(data[key][:train_split])\n",
    "        # tensor_train, min_value, max_value = min_max_scaler(tensor_train, train_split)\n",
    "        tensor_train = get_sequence(tensor_train, sequence_length)\n",
    "        feature_train.append(tensor_train)\n",
    "        \n",
    "        tensor_test = tf.convert_to_tensor(data[key][train_split:])\n",
    "        #tensor_test = tf.math.divide(tf.math.subtract(tensor_test, min_value),tf.math.subtract(max_value, min_value))\n",
    "        tensor_test = get_sequence(tensor_test, sequence_length)\n",
    "        feature_test.append(tensor_test)\n",
    "        \n",
    "        # scaler_parameter[key] = [min_value, max_value]\n",
    "    feature_train = tf.concat([feature_train[_] for _ in range(len(feature_train))], axis = 2)\n",
    "    feature_test = tf.concat([feature_test[_] for _ in range(len(feature_test))], axis = 2)\n",
    "    \n",
    "    tensor_train = data[label_column][:train_split]\n",
    "    # tensor_train, min_value, max_value = min_max_scaler(tensor_train, train_split)\n",
    "    label_train = tf.convert_to_tensor(get_label(tensor_train, sequence_length))\n",
    "    \n",
    "    tensor_test = data[label_column][train_split:]\n",
    "    # tensor_test = tf.math.divide(tf.math.subtract(tensor_test, min_value), tf.math.subtract(max_value, min_value))\n",
    "    label_test = tf.convert_to_tensor(get_label(tensor_test, sequence_length))\n",
    "    \n",
    "    # scaler_parameter[label_column] = [min_value, max_value]\n",
    "    # label = min_max_scaler(label_tensor)\n",
    "    \n",
    "    return feature_train, feature_test, label_train, label_test #, scaler_parameter\n",
    "\n",
    "def tensorflow_transform_for_encoder(feature, label, buffer_size, batch_size):\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((feature, label))\n",
    "    dataset = dataset.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
    "    tensor_predict = feature\n",
    "    \n",
    "    return dataset, tensor_predict\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    # Read data from csv.file and generate sequence ready for training\n",
    "    data_file_path = \"gs://{}/cost_anomaly_detection/preprocess/{}*{}*\".format(bucket, \"cost\", pattern)\n",
    "    data_dict = read_data_csv(data_file_path)\n",
    "    feature_train, feature_test, label_train, label_test = preprocess_data(\n",
    "        data_dict, feature_columns, label_column, train_split, sequence_length\n",
    "    )\n",
    "    # Convert sequence to tensorflow dataset\n",
    "    dataset_train, tensor_to_extract = tensorflow_transform_for_encoder(\n",
    "        feature_train, feature_train, buffer_size, batch_size\n",
    "    )\n",
    "    dataset_test, tensor_to_eval = tensorflow_transform_for_encoder(\n",
    "        feature_test, feature_test, buffer_size, batch_size\n",
    "    )\n",
    "    \n",
    "    return dataset_train, dataset_train, tensor_to_extract, tensor_to_eval, label_train, label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/model.py\n",
    "\n",
    "import shutil, os, datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "bucket = None  # set from task.py\n",
    "pattern = \"of\" # gets all files\n",
    "\n",
    "# Define some hyperparamters\n",
    "epochs_encoder = 100\n",
    "epochs_forecaster = 50\n",
    "validation_steps = 50\n",
    "sequence_length = 7\n",
    "extract_mode = \"raw\"\n",
    "num_features = len(feature_columns)\n",
    "batch_size = 5\n",
    "buffer_size = 1000\n",
    "dropout = 0.3\n",
    "encode_wide = [128]\n",
    "decode_wide = [32]\n",
    "num_extracts = num_features + encode_wide[0] if extract_mode == \"raw\" else num_features + 1\n",
    "forecaster_wide = [400, 200]\n",
    "dense_size = 200\n",
    "evaluation_interval = 200\n",
    "\n",
    "# Create a function for build a LSTM Autoencoder\n",
    "def build_lstm_autoencoder(sequence_length, num_features, dropout, encode_wide, decode_wide):\n",
    "    \n",
    "    # Neuron Architecture\n",
    "    # Input Layer\n",
    "    input_layer = Input(shape = (sequence_length, num_features))\n",
    "    # Encoder Layer\n",
    "    for deep, wide in enumerate(encode_wide):\n",
    "        if deep == 0:\n",
    "            encoded_layer = LSTM(wide, return_sequences = True)(input_layer, training = True)\n",
    "            # encoded_layer = BatchNormalization()(encoded_layer, training = True)\n",
    "            encoded_layer = Dropout(dropout)(encoded_layer, training = True)       \n",
    "        else:\n",
    "            encoded_layer = LSTM(wide, return_sequences = True)(encoded_layer, training = True)\n",
    "            # encoded_layer = BatchNormalization()(encoded_layer, training = True)\n",
    "            encoded_layer = Dropout(dropout)(encoded_layer, training = True)\n",
    "    # Decoder Layer\n",
    "    for deep, wide in enumerate(decode_wide):\n",
    "        if deep == 0:\n",
    "            decoded_layer = LSTM(wide, return_sequences = True)(encoded_layer, training = True)\n",
    "            # decoded_layer = BatchNormalization()(decoded_layer, training = True)\n",
    "            decoded_layer = Dropout(dropout)(decoded_layer, training = True)\n",
    "        else:\n",
    "            decoded_layer = LSTM(wide, return_sequences = True)(decoded_layer, training = True)\n",
    "            # decoded_layer = BatchNormalization()(decoded_layer, training = True)\n",
    "            decoded_layer = Dropout(dropout)(decoded_layer, training = True)\n",
    "    # Output Layer\n",
    "    output_layer = TimeDistributed(Dense(2))(decoded_layer)\n",
    "    \n",
    "    lstm_encoder = Model(input_layer, encoded_layer)\n",
    "    lstm_autoencoder = Model(input_layer, output_layer)\n",
    "    lstm_autoencoder.compile(optimizer = RMSprop(), loss = \"mean_squared_error\")\n",
    "    \n",
    "    return lstm_encoder, lstm_autoencoder\n",
    "\n",
    "def train_lstm_autoencoder(dataset_train, dataset_test, output_dir):\n",
    "\n",
    "    lstm_encoder, lstm_autoencoder = build_lstm_autoencoder(\n",
    "        sequence_length, num_features, dropout, encode_wide, decode_wide\n",
    "    )\n",
    "    print(\"Here is our LSTM-Autoencoder architecture so far:\\n\")\n",
    "    lstm_autoencoder.summary()\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    log_path = os.path.join(output_dir, \"logs_autoencoder\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_path)\n",
    "    lstm_autoencoder.fit(\n",
    "        dataset_train, epochs = epochs_encoder, steps_per_epoch = evaluation_interval, \n",
    "        validation_data = dataset_test, validation_steps = validation_steps, verbose = 2, \n",
    "        callbacks = [tensorboard_callback]\n",
    "    )\n",
    "    \n",
    "    export_path = os.path.join(output_dir, \"lstm_encoder\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tf.saved_model.save(lstm_encoder, export_path)\n",
    "    print(\"Saved LSTM-Encoder to {}\".format(export_path))\n",
    "\n",
    "# Create a function to extract features from LSTM encoder and prepare them for LSTM forecaster\n",
    "def ensemble_averaging(feature):\n",
    "    \n",
    "    feature_extract = np.array(\n",
    "        [tf.math.reduce_mean(feature[_1, _2, :]) for _1 in range(feature.shape[0]) for _2 in range(feature.shape[1])]\n",
    "    )\n",
    "    feature_extract = tf.convert_to_tensor(feature_extract.reshape((feature.shape[0], feature.shape[1], 1)))\n",
    "    \n",
    "    return feature_extract\n",
    "    \n",
    "def sequence_extraction(encoder, feature, sequence_length, mode):\n",
    "    \n",
    "    feature_encoded = encoder.predict(feature)\n",
    "    feature_ensemble = ensemble_averaging(feature)\n",
    "    \n",
    "    # Encoder will generate multiple sequences from original one.\n",
    "    # We can directly use the raw sequences generated from encoder Or use the ensembled sequence (averaging).\n",
    "    # We can also consider whether includes fixtures in the forecaster.\n",
    "    if mode == \"raw\":\n",
    "        feature_extract = tf.concat([feature_encoded, feature], axis = 2)     \n",
    "    else:\n",
    "        feature_extract = tf.concat([feature_ensemble, feature], axis = 2)       \n",
    "    \n",
    "    return feature_extract\n",
    "\n",
    "def tensorflow_transform_for_forecaster(feature, label, encoder, buffer_size, batch_size):\n",
    "    \n",
    "    feature_extract = sequence_extraction(\n",
    "        encoder, feature = feature, sequence_length = sequence_length, mode = extract_mode\n",
    "    )\n",
    "    dataset, tensor_predict = tensorflow_transform_for_encoder(feature_extract, label, buffer_size, batch_size)\n",
    "    \n",
    "    return dataset, tensor_predict\n",
    "\n",
    "# Create a function to build LSTM forecaster\n",
    "def rmse(y, y_pred):\n",
    "    return tf.sqrt(tf.math.reduce_mean(tf.math.square(y_pred - y))) \n",
    "\n",
    "def build_lstm_forecaster(dropout, num_extracts, forecaster_wide, dense_size):\n",
    "    \n",
    "    ### Neuron Architecture\n",
    "    # Input Layer\n",
    "    input_lstm = Input(shape = (sequence_length, num_extracts))\n",
    "    # LSTM Layer\n",
    "    if len(forecaster_wide) == 1:\n",
    "        lstm = LSTM(forecaster_wide[0], return_sequences = False)(input_lstm, training = True)\n",
    "        # lstm = BatchNormalization()(lstm, training = True)\n",
    "        lstm = Dropout(dropout)(lstm, training = True)\n",
    "    else:\n",
    "        for deep, wide in enumerate(forecaster_wide):\n",
    "            if deep == 0:\n",
    "                lstm = LSTM(wide, return_sequences = True)(input_lstm, training = True)\n",
    "                # lstm = BatchNormalization()(lstm, training = True)\n",
    "                lstm = Dropout(dropout)(lstm, training = True)\n",
    "            elif deep == len(forecaster_wide) - 1:\n",
    "                lstm = LSTM(wide, return_sequences = False)(lstm, training = True)\n",
    "                # lstm = BatchNormalization()(lstm, training = True)\n",
    "                lstm = Dropout(dropout)(lstm, training = True)\n",
    "            else:\n",
    "                lstm = LSTM(wide, return_sequences = True)(lstm, training = True)\n",
    "                # lstm = BatchNormalization()(lstm, training = True)\n",
    "                lstm = Dropout(dropout)(lstm, training = True)\n",
    "    # Output Layer\n",
    "    dense = Dense(dense_size)(lstm)\n",
    "    output_lstm = Dense(1)(dense)\n",
    "    \n",
    "    forecaster = Model(input_lstm, output_lstm)\n",
    "    forecaster.compile(optimizer = RMSprop(), loss = \"mean_squared_error\", metrics = [rmse, \"mse\"])\n",
    "    \n",
    "    return forecaster\n",
    "\n",
    "def train_lstm_forecaster(dataset_train, dataset_test, output_dir):\n",
    "    \n",
    "    lstm_forecaster = build_lstm_forecaster(\n",
    "        dropout, num_extracts, forecaster_wide, dense_size\n",
    "    )\n",
    "    \n",
    "    print(\"Here is our LSTM-Forecaster architecture so far:\\n\")\n",
    "    lstm_forecaster.summary()\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    log_path = os.path.join(output_dir, \"logs_forecaster\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_path)\n",
    "    lstm_forecaster.fit(\n",
    "        dataset_train, epochs = epochs_forecaster, steps_per_epoch = evaluation_interval, \n",
    "        validation_data = dataset_test, verbose = 2, validation_steps = validation_steps,\n",
    "        callbacks = [tensorboard_callback]\n",
    "    )\n",
    "    export_path = os.path.join(output_dir, \"lstm_forecaster\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tf.saved_model.save(lstm_forecaster, export_path)\n",
    "    print(\"Saved LSTM-Forecaster to {}\".format(export_path))\n",
    "\n",
    "\n",
    "def deploy_lstm_autoencoder(dataset_train, dataset_test, output_dir):\n",
    "    \n",
    "    # Build LSTM-Autoencoder and train the encoder\n",
    "    lstm_encoder, lstm_autoencoder = build_lstm_autoencoder(\n",
    "        sequence_length, num_features, dropout, encode_wide, decode_wide\n",
    "    )\n",
    "    train_lstm_autoencoder(dataset_train, dataset_test, output_dir)\n",
    "    \n",
    "    return lstm_encoder\n",
    "    \n",
    "def deploy_lstm_forecaster(tensor_to_extract, tensor_to_eval, encoder, label_train, label_test, output_dir):\n",
    "    \n",
    "    # Generate extracted features from raw features and convert them to tensorflow dataset\n",
    "    dataset_train, _ = tensorflow_transform_for_forecaster(\n",
    "        tensor_to_extract, label_train, lstm_encoder, buffer_size, batch_size\n",
    "    )\n",
    "    dataset_test, _ = tensorflow_transform_for_forecaster(\n",
    "        tensor_to_eval, label_test, lstm_encoder, buffer_size, batch_size\n",
    "    )\n",
    "    # Build LSTM-Forecaster and train the forecaster\n",
    "    lstm_forecaster = build_lstm_forecaster(\n",
    "        dropout, num_extracts, forecaster_wide, dense_size\n",
    "    )\n",
    "    train_lstm_forecaster(dataset_train, dataset_test, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud ai-platform local train \\\n",
    "  --package-path trainer \\\n",
    "  --module-name trainer.task \\\n",
    "  --job-dir local-training-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cost_anomaly_detection/preproc/cost_null_project.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/preproc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model on GCP AI platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (gcloud.ai-platform.jobs.submit.training) unrecognized arguments:\n",
      "  trainer/\n",
      "  --bucket=cost_anomaly_detection (did you mean '--staging-bucket'?)\n",
      "  To search the help text of gcloud commands, run:\n",
      "  gcloud help -- SEARCH_TERMS\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'JOB_NAME=\"anomaly_lstm_$(date +%Y%m%d_%H%M%S)\"\\nBUCKET=\"cost_anomaly_detection\"\\nOUTDIR=\"gs://$BUCKET/$JOB_NAME\"\\nTFVERSION=\"2.1\"\\nREGION=\"europe-west1\"\\ngcloud ai-platform jobs submit training $JOB_NAME \\\\\\n    --module-name=trainer.task \\\\\\n    --package-path= trainer/ \\\\\\n    --region=$REGION \\\\\\n    --bucket=$BUCKET \\\\\\n    --job-dir=$OUTDIR \\\\\\n    --runtime-version=$TFVERSION \\\\\\n    -- \\\\\\n    --output_dir=$OUTDIR \\\\\\n    --stream-logs\\n'' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bdea087dc35c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'JOB_NAME=\"anomaly_lstm_$(date +%Y%m%d_%H%M%S)\"\\nBUCKET=\"cost_anomaly_detection\"\\nOUTDIR=\"gs://$BUCKET/$JOB_NAME\"\\nTFVERSION=\"2.1\"\\nREGION=\"europe-west1\"\\ngcloud ai-platform jobs submit training $JOB_NAME \\\\\\n    --module-name=trainer.task \\\\\\n    --package-path= trainer/ \\\\\\n    --region=$REGION \\\\\\n    --bucket=$BUCKET \\\\\\n    --job-dir=$OUTDIR \\\\\\n    --runtime-version=$TFVERSION \\\\\\n    -- \\\\\\n    --output_dir=$OUTDIR \\\\\\n    --stream-logs\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Work/cost_anomaly_detection/tensorflow-venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Work/cost_anomaly_detection/tensorflow-venv/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/Documents/Work/cost_anomaly_detection/tensorflow-venv/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Work/cost_anomaly_detection/tensorflow-venv/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'JOB_NAME=\"anomaly_lstm_$(date +%Y%m%d_%H%M%S)\"\\nBUCKET=\"cost_anomaly_detection\"\\nOUTDIR=\"gs://$BUCKET/$JOB_NAME\"\\nTFVERSION=\"2.1\"\\nREGION=\"europe-west1\"\\ngcloud ai-platform jobs submit training $JOB_NAME \\\\\\n    --module-name=trainer.task \\\\\\n    --package-path= trainer/ \\\\\\n    --region=$REGION \\\\\\n    --bucket=$BUCKET \\\\\\n    --job-dir=$OUTDIR \\\\\\n    --runtime-version=$TFVERSION \\\\\\n    -- \\\\\\n    --output_dir=$OUTDIR \\\\\\n    --stream-logs\\n'' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "JOB_NAME=\"anomaly_lstm_$(date +%Y%m%d_%H%M%S)\"\n",
    "BUCKET=\"cost_anomaly_detection\"\n",
    "OUTDIR=\"gs://$BUCKET/$JOB_NAME\"\n",
    "TFVERSION=\"2.1\"\n",
    "REGION=\"europe-west1\"\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path= trainer/ \\\n",
    "    --region=$REGION \\\n",
    "    --bucket=$BUCKET \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --stream-logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
